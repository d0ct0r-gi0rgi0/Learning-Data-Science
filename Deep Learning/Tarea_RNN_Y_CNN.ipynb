{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-america",
   "metadata": {
    "id": "prospective-america"
   },
   "source": [
    "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
    "\n",
    "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
    "\n",
    "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-stewart",
   "metadata": {
    "id": "novel-stewart"
   },
   "source": [
    "El examen se divide en tres partes, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
    "\n",
    "    \n",
    "- [Actividad 1: Redes Recurrentes](#actividad_1): 10 pts\n",
    "    - [Cuestión 1](#3.1): 2.5 pt\n",
    "    - [Cuestión 2](#3.2): 2.5 pt\n",
    "    - [Cuestión 3](#3.3): 2.5 pts\n",
    "    - [Cuestión 4](#3.4): 1.25 pts\n",
    "    - [Cuestión 5](#3.5): 1.25 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prompt-developer",
   "metadata": {
    "id": "prompt-developer"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-favorite",
   "metadata": {
    "id": "regional-favorite"
   },
   "source": [
    "<a name='actividad_1'></a>\n",
    "# Actividad 1: Redes Recurrentes\n",
    "\n",
    "\n",
    "- [Cuestión 1](#3.1): 2.5 pt\n",
    "- [Cuestión 2](#3.2): 2.5 pt\n",
    "- [Cuestión 3](#3.3): 2.5 pts\n",
    "- [Cuestión 4](#3.4): 1.25 pts\n",
    "- [Cuestión 5](#3.5): 1.25 pts\n",
    "\n",
    "Vamos a usar un dataset de las temperaturas mínimas diarias en Melbourne. La tarea será la de predecir la temperatura mínima en dos días. Puedes usar técnicas de series temporales vistas en otras asignaturas, pero no es necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "empty-value",
   "metadata": {
    "id": "empty-value"
   },
   "outputs": [],
   "source": [
    "dataset_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
    "data_dir = tf.keras.utils.get_file('daily-min-temperatures.csv', origin=dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "numerous-situation",
   "metadata": {
    "id": "numerous-situation"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Temp",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d1b1f25e-f8d2-46bc-a6a0-3963ca85063e",
       "rows": [
        [
         "0",
         "1981-01-01 00:00:00",
         "20.7"
        ],
        [
         "1",
         "1981-01-02 00:00:00",
         "17.9"
        ],
        [
         "2",
         "1981-01-03 00:00:00",
         "18.8"
        ],
        [
         "3",
         "1981-01-04 00:00:00",
         "14.6"
        ],
        [
         "4",
         "1981-01-05 00:00:00",
         "15.8"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-02</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-03</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-04</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Temp\n",
       "0 1981-01-01  20.7\n",
       "1 1981-01-02  17.9\n",
       "2 1981-01-03  18.8\n",
       "3 1981-01-04  14.6\n",
       "4 1981-01-05  15.8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir, parse_dates=['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "copyrighted-madonna",
   "metadata": {
    "id": "copyrighted-madonna"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3650\n",
      "number of train samples: 3000\n",
      "number of test samples: 650\n",
      "firsts train samples: [20.7 17.9 18.8 14.6 15.8 15.8 15.8 17.4 21.8 20. ]\n"
     ]
    }
   ],
   "source": [
    "temperatures = df['Temp'].values\n",
    "print('number of samples:', len(temperatures))\n",
    "train_data = temperatures[:3000]\n",
    "test_data = temperatures[3000:]\n",
    "print('number of train samples:', len(train_data))\n",
    "print('number of test samples:', len(test_data))\n",
    "print('firsts train samples:', train_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-brief",
   "metadata": {
    "id": "adapted-brief"
   },
   "source": [
    "<a name='3.1'></a>\n",
    "## Cuestión 1: Convierta `train_data` y `test_data`  en ventanas de tamaño 5, para predecir el valor en 2 días\n",
    "\n",
    "En la nomenclatura de [Introduction_to_RNN_Time_Series.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/RNN/Introduction_to_RNN_Time_Series.ipynb)\n",
    "```python\n",
    "past, future = (5, 2)\n",
    "```\n",
    "\n",
    "Para las primeras 10 muestras de train_data `[20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20. ]` el resultado debería ser:\n",
    "\n",
    "```python\n",
    "x[0] : [20.7, 17.9, 18.8, 14.6, 15.8] , y[0]: 15.8\n",
    "x[1] : [17.9, 18.8, 14.6, 15.8, 15.8] , y[1]: 17.4\n",
    "x[2] : [18.8, 14.6, 15.8, 15.8, 15.8] , y[2]: 21.8\n",
    "x[3] : [14.6, 15.8, 15.8, 15.8, 17.4] , y[3]: 20.             \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "conscious-teaching",
   "metadata": {
    "id": "conscious-teaching"
   },
   "outputs": [],
   "source": [
    "# windowing function\n",
    "def create_windows_np(data, window_size, horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size+horizon-1])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "joint-annotation",
   "metadata": {
    "id": "joint-annotation"
   },
   "outputs": [],
   "source": [
    "past, future = (5, 2)\n",
    "X_train, y_train = create_windows_np(data = train_data, window_size = past, horizon = future)\n",
    "X_test, y_test = create_windows_np(data = test_data, window_size = past, horizon = future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-junior",
   "metadata": {
    "id": "electrical-junior"
   },
   "source": [
    "<a name='3.2'></a>\n",
    "## Cuestión 2: Cree un modelo recurrente de dos capas GRU para predecir con las ventanas de la cuestión anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aboriginal-complaint",
   "metadata": {
    "id": "aboriginal-complaint"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU1 (\u001b[38;5;33mGRU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m12,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU2 (\u001b[38;5;33mGRU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m9,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,305</span> (87.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,305\u001b[0m (87.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,305</span> (87.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,305\u001b[0m (87.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = keras.layers.Input(shape = (past, 1), name = 'Input')\n",
    "\n",
    "# Capas GRU:\n",
    "gru_1 = keras.layers.GRU(64, return_sequences = True, name = 'GRU1')(inputs)\n",
    "gru_2 = keras.layers.GRU(32, name = 'GRU2')(gru_1)\n",
    "\n",
    "# Outputs:\n",
    "outputs = keras.layers.Dense(1, name = 'Output')(gru_2)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = 'mse',\n",
    "    metrics = ['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "applicable-longer",
   "metadata": {
    "id": "applicable-longer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 87.9182 - mae: 8.2154 - val_loss: 33.5168 - val_mae: 4.7416\n",
      "Epoch 2/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24.1005 - mae: 3.9356 - val_loss: 24.4244 - val_mae: 3.9572\n",
      "Epoch 3/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.8102 - mae: 3.5203 - val_loss: 20.7139 - val_mae: 3.6389\n",
      "Epoch 4/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 17.6116 - mae: 3.3354 - val_loss: 18.6696 - val_mae: 3.4542\n",
      "Epoch 5/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15.5370 - mae: 3.1122 - val_loss: 16.6357 - val_mae: 3.2128\n",
      "Epoch 6/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.5547 - mae: 2.8722 - val_loss: 14.3119 - val_mae: 2.9613\n",
      "Epoch 7/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12.1728 - mae: 2.7117 - val_loss: 12.5872 - val_mae: 2.7594\n",
      "Epoch 8/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.7400 - mae: 2.6515 - val_loss: 11.3962 - val_mae: 2.6194\n",
      "Epoch 9/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.2361 - mae: 2.4943 - val_loss: 10.5040 - val_mae: 2.5183\n",
      "Epoch 10/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 11.0573 - mae: 2.5822 - val_loss: 10.1378 - val_mae: 2.4545\n",
      "Epoch 11/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.8820 - mae: 2.4267 - val_loss: 9.9055 - val_mae: 2.4254\n",
      "Epoch 12/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.3031 - mae: 2.3699 - val_loss: 9.3894 - val_mae: 2.3610\n",
      "Epoch 13/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.3979 - mae: 2.3890 - val_loss: 9.2324 - val_mae: 2.3348\n",
      "Epoch 14/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.8992 - mae: 2.3154 - val_loss: 9.1144 - val_mae: 2.3205\n",
      "Epoch 15/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.6093 - mae: 2.3977 - val_loss: 8.8008 - val_mae: 2.2828\n",
      "Epoch 16/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8628 - mae: 2.3537 - val_loss: 8.9819 - val_mae: 2.3031\n",
      "Epoch 17/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.8424 - mae: 2.3356 - val_loss: 8.8004 - val_mae: 2.2855\n",
      "Epoch 18/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1532 - mae: 2.3572 - val_loss: 8.6294 - val_mae: 2.2626\n",
      "Epoch 19/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5939 - mae: 2.3063 - val_loss: 8.7112 - val_mae: 2.2672\n",
      "Epoch 20/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4901 - mae: 2.2994 - val_loss: 8.4962 - val_mae: 2.2479\n",
      "Epoch 21/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7617 - mae: 2.2994 - val_loss: 9.3297 - val_mae: 2.3470\n",
      "Epoch 22/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.4228 - mae: 2.3802 - val_loss: 8.4273 - val_mae: 2.2380\n",
      "Epoch 23/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.7998 - mae: 2.3374 - val_loss: 8.5532 - val_mae: 2.2562\n",
      "Epoch 24/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.8657 - mae: 2.3194 - val_loss: 8.4177 - val_mae: 2.2324\n",
      "Epoch 25/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.9435 - mae: 2.3453 - val_loss: 8.4678 - val_mae: 2.2405\n",
      "Epoch 26/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.5700 - mae: 2.3006 - val_loss: 8.3506 - val_mae: 2.2253\n",
      "Epoch 27/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.2218 - mae: 2.2552 - val_loss: 8.3399 - val_mae: 2.2293\n",
      "Epoch 28/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 8.5540 - mae: 2.3081 - val_loss: 8.3345 - val_mae: 2.2220\n",
      "Epoch 29/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.1324 - mae: 2.3552 - val_loss: 8.6126 - val_mae: 2.2551\n",
      "Epoch 30/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5480 - mae: 2.2745 - val_loss: 8.4775 - val_mae: 2.2406\n",
      "Epoch 31/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.4229 - mae: 2.2552 - val_loss: 8.4440 - val_mae: 2.2362\n",
      "Epoch 32/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.4291 - mae: 2.2681 - val_loss: 8.3027 - val_mae: 2.2256\n",
      "Epoch 33/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5989 - mae: 2.2892 - val_loss: 8.3447 - val_mae: 2.2211\n",
      "Epoch 34/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.2651 - mae: 2.2394 - val_loss: 9.0955 - val_mae: 2.3184\n",
      "Epoch 35/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 8.7708 - mae: 2.3106 - val_loss: 8.3016 - val_mae: 2.2240\n",
      "Epoch 36/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5440 - mae: 2.2839 - val_loss: 8.3014 - val_mae: 2.2155\n",
      "Epoch 37/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4162 - mae: 2.2758 - val_loss: 8.3085 - val_mae: 2.2182\n",
      "Epoch 38/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5571 - mae: 2.2988 - val_loss: 8.2513 - val_mae: 2.2135\n",
      "Epoch 39/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8464 - mae: 2.3357 - val_loss: 8.4882 - val_mae: 2.2359\n",
      "Epoch 40/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4013 - mae: 2.2523 - val_loss: 8.4045 - val_mae: 2.2268\n",
      "Epoch 41/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7767 - mae: 2.3174 - val_loss: 8.4680 - val_mae: 2.2486\n",
      "Epoch 42/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.8894 - mae: 2.3446 - val_loss: 8.6397 - val_mae: 2.2562\n",
      "Epoch 43/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.5437 - mae: 2.2849 - val_loss: 8.3221 - val_mae: 2.2188\n",
      "Epoch 44/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.9464 - mae: 2.3416 - val_loss: 8.6272 - val_mae: 2.2601\n",
      "Epoch 45/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.3057 - mae: 2.4073 - val_loss: 8.3647 - val_mae: 2.2238\n",
      "Epoch 46/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5368 - mae: 2.2700 - val_loss: 8.2685 - val_mae: 2.2197\n",
      "Epoch 47/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6729 - mae: 2.2994 - val_loss: 8.7455 - val_mae: 2.2823\n",
      "Epoch 48/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.3789 - mae: 2.2393 - val_loss: 8.2399 - val_mae: 2.2109\n",
      "Epoch 49/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0167 - mae: 2.2213 - val_loss: 8.2525 - val_mae: 2.2194\n",
      "Epoch 50/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.4589 - mae: 2.2994 - val_loss: 8.4668 - val_mae: 2.2471\n",
      "Epoch 51/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7425 - mae: 2.3138 - val_loss: 8.1956 - val_mae: 2.1993\n",
      "Epoch 52/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9132 - mae: 2.3220 - val_loss: 8.3987 - val_mae: 2.2186\n",
      "Epoch 53/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0470 - mae: 2.2065 - val_loss: 8.2513 - val_mae: 2.2180\n",
      "Epoch 54/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4655 - mae: 2.2672 - val_loss: 8.2718 - val_mae: 2.2190\n",
      "Epoch 55/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.5155 - mae: 2.2695 - val_loss: 8.8925 - val_mae: 2.2986\n",
      "Epoch 56/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4414 - mae: 2.2516 - val_loss: 8.2578 - val_mae: 2.2066\n",
      "Epoch 57/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4581 - mae: 2.2786 - val_loss: 8.1685 - val_mae: 2.2037\n",
      "Epoch 58/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.3454 - mae: 2.2661 - val_loss: 8.5840 - val_mae: 2.2456\n",
      "Epoch 59/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0552 - mae: 2.2068 - val_loss: 8.6127 - val_mae: 2.2568\n",
      "Epoch 60/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7248 - mae: 2.3135 - val_loss: 8.2366 - val_mae: 2.2158\n",
      "Epoch 61/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2383 - mae: 2.2382 - val_loss: 8.4258 - val_mae: 2.2305\n",
      "Epoch 62/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6949 - mae: 2.2901 - val_loss: 8.1588 - val_mae: 2.1978\n",
      "Epoch 63/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6739 - mae: 2.3034 - val_loss: 8.1482 - val_mae: 2.1977\n",
      "Epoch 64/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.6073 - mae: 2.2998 - val_loss: 8.1247 - val_mae: 2.1923\n",
      "Epoch 65/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6120 - mae: 2.2999 - val_loss: 8.4945 - val_mae: 2.2460\n",
      "Epoch 66/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.5772 - mae: 2.2810 - val_loss: 8.1897 - val_mae: 2.2086\n",
      "Epoch 67/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6473 - mae: 2.2903 - val_loss: 8.3349 - val_mae: 2.2280\n",
      "Epoch 68/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7652 - mae: 2.3134 - val_loss: 8.3978 - val_mae: 2.2305\n",
      "Epoch 69/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.6379 - mae: 2.2953 - val_loss: 8.4909 - val_mae: 2.2539\n",
      "Epoch 70/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.4631 - mae: 2.2613 - val_loss: 8.1644 - val_mae: 2.2113\n",
      "Epoch 71/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5362 - mae: 2.3011 - val_loss: 8.2732 - val_mae: 2.2180\n",
      "Epoch 72/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.7037 - mae: 2.2970 - val_loss: 8.3042 - val_mae: 2.2299\n",
      "Epoch 73/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1010 - mae: 2.2211 - val_loss: 8.4340 - val_mae: 2.2389\n",
      "Epoch 74/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.4337 - mae: 2.2640 - val_loss: 8.1827 - val_mae: 2.2191\n"
     ]
    }
   ],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=10)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "stone-province",
   "metadata": {
    "id": "stone-province"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9076 - mae: 2.0529\n",
      "Test Loss: [7.037185192108154, 2.0682549476623535]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-guitar",
   "metadata": {
    "id": "genetic-guitar"
   },
   "source": [
    "<a name='3.3'></a>\n",
    "## Cuestión 3: Añada más features a la series temporal, por ejemplo `portion_year`. Cree un modelo que mejore al anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "prospective-master",
   "metadata": {
    "id": "prospective-master"
   },
   "outputs": [],
   "source": [
    "## Puede añadir más features\n",
    "df['portion_year'] = df['Date'].dt.dayofyear / 365.0\n",
    "df_multi = df[['Temp', 'portion_year']].copy()\n",
    "\n",
    "## train - test split\n",
    "train_data = df_multi.iloc[:3000].copy()\n",
    "test_data = df_multi.loc[3000:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "threaded-sheriff",
   "metadata": {
    "id": "threaded-sheriff"
   },
   "outputs": [],
   "source": [
    "## Create windows\n",
    "\n",
    "# Multivariate windowing function:\n",
    "def create_windows_multivariate_np(data, window_size, horizon, target_col_idx):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i+window_size, :])\n",
    "        y.append(data[i+window_size+horizon-1, target_col_idx])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = create_windows_multivariate_np(data = train_data, window_size = past, horizon = future, target_col_idx = 0)\n",
    "X_test, y_test = create_windows_multivariate_np(data = test_data, window_size = past, horizon = future, target_col_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "stable-estate",
   "metadata": {
    "id": "stable-estate"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU1 (\u001b[38;5;33mGRU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m13,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU2 (\u001b[38;5;33mGRU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m9,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,497</span> (87.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,497\u001b[0m (87.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,497</span> (87.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,497\u001b[0m (87.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inputs:\n",
    "inputs = keras.layers.Input(shape = (past, 2), name = 'Input')\n",
    "\n",
    "# Capas GRU:\n",
    "gru_1 = keras.layers.GRU(64, return_sequences = True, name = 'GRU1')(inputs)\n",
    "gru_2 = keras.layers.GRU(32, name = 'GRU2')(gru_1)\n",
    "\n",
    "# Outputs:\n",
    "outputs = keras.layers.Dense(1, name = 'Output')(gru_2)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = 'mse',\n",
    "    metrics = ['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "structured-philip",
   "metadata": {
    "id": "structured-philip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - loss: 92.2403 - mae: 8.5536 - val_loss: 40.2540 - val_mae: 5.2919\n",
      "Epoch 2/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 28.0686 - mae: 4.2770 - val_loss: 28.2198 - val_mae: 4.2818\n",
      "Epoch 3/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.3238 - mae: 3.6721 - val_loss: 22.9944 - val_mae: 3.8329\n",
      "Epoch 4/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 17.3599 - mae: 3.3015 - val_loss: 20.0261 - val_mae: 3.5830\n",
      "Epoch 5/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17.3383 - mae: 3.2986 - val_loss: 18.3912 - val_mae: 3.4316\n",
      "Epoch 6/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 16.4021 - mae: 3.1996 - val_loss: 15.8521 - val_mae: 3.1257\n",
      "Epoch 7/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.7165 - mae: 2.8516 - val_loss: 13.9335 - val_mae: 2.9090\n",
      "Epoch 8/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 11.8359 - mae: 2.6878 - val_loss: 13.0271 - val_mae: 2.8057\n",
      "Epoch 9/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.5485 - mae: 2.5328 - val_loss: 11.7530 - val_mae: 2.6553\n",
      "Epoch 10/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 10.3406 - mae: 2.4878 - val_loss: 10.6067 - val_mae: 2.5103\n",
      "Epoch 11/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.8197 - mae: 2.4130 - val_loss: 10.5279 - val_mae: 2.5172\n",
      "Epoch 12/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.5024 - mae: 2.4089 - val_loss: 9.9529 - val_mae: 2.4365\n",
      "Epoch 13/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.7493 - mae: 2.4081 - val_loss: 9.5824 - val_mae: 2.3829\n",
      "Epoch 14/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.3227 - mae: 2.3670 - val_loss: 9.1988 - val_mae: 2.3293\n",
      "Epoch 15/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.5806 - mae: 2.4197 - val_loss: 9.2263 - val_mae: 2.3403\n",
      "Epoch 16/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8640 - mae: 2.3169 - val_loss: 8.8785 - val_mae: 2.2941\n",
      "Epoch 17/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4185 - mae: 2.2659 - val_loss: 8.7983 - val_mae: 2.2802\n",
      "Epoch 18/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8178 - mae: 2.3194 - val_loss: 8.7443 - val_mae: 2.2750\n",
      "Epoch 19/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9476 - mae: 2.3357 - val_loss: 8.6899 - val_mae: 2.2697\n",
      "Epoch 20/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1566 - mae: 2.3451 - val_loss: 8.9291 - val_mae: 2.2915\n",
      "Epoch 21/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.7329 - mae: 2.2713 - val_loss: 8.4995 - val_mae: 2.2445\n",
      "Epoch 22/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.9938 - mae: 2.3383 - val_loss: 8.6011 - val_mae: 2.2602\n",
      "Epoch 23/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1981 - mae: 2.3597 - val_loss: 8.8435 - val_mae: 2.2853\n",
      "Epoch 24/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.2138 - mae: 2.3768 - val_loss: 9.0352 - val_mae: 2.3143\n",
      "Epoch 25/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.6219 - mae: 2.2878 - val_loss: 8.7353 - val_mae: 2.2851\n",
      "Epoch 26/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.1872 - mae: 2.3584 - val_loss: 8.5106 - val_mae: 2.2476\n",
      "Epoch 27/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.0539 - mae: 2.3610 - val_loss: 8.5047 - val_mae: 2.2369\n",
      "Epoch 28/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5923 - mae: 2.2898 - val_loss: 8.5394 - val_mae: 2.2476\n",
      "Epoch 29/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.6776 - mae: 2.2863 - val_loss: 8.5322 - val_mae: 2.2441\n",
      "Epoch 30/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.8994 - mae: 2.3341 - val_loss: 8.5276 - val_mae: 2.2410\n",
      "Epoch 31/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6065 - mae: 2.2879 - val_loss: 8.3554 - val_mae: 2.2204\n",
      "Epoch 32/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2782 - mae: 2.2442 - val_loss: 8.5061 - val_mae: 2.2400\n",
      "Epoch 33/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9.3067 - mae: 2.3604 - val_loss: 8.6405 - val_mae: 2.2635\n",
      "Epoch 34/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.8739 - mae: 2.3125 - val_loss: 8.3914 - val_mae: 2.2224\n",
      "Epoch 35/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9528 - mae: 2.2262 - val_loss: 8.2805 - val_mae: 2.2185\n",
      "Epoch 36/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.5559 - mae: 2.2912 - val_loss: 8.3797 - val_mae: 2.2235\n",
      "Epoch 37/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.8126 - mae: 2.3355 - val_loss: 8.7065 - val_mae: 2.2541\n",
      "Epoch 38/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6404 - mae: 2.3115 - val_loss: 8.5738 - val_mae: 2.2422\n",
      "Epoch 39/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0861 - mae: 2.2164 - val_loss: 8.4021 - val_mae: 2.2189\n",
      "Epoch 40/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.2632 - mae: 2.2490 - val_loss: 8.2991 - val_mae: 2.2100\n",
      "Epoch 41/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 9.0178 - mae: 2.3430 - val_loss: 8.3058 - val_mae: 2.2251\n",
      "Epoch 42/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.9296 - mae: 2.2149 - val_loss: 8.2839 - val_mae: 2.2100\n",
      "Epoch 43/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6245 - mae: 2.2667 - val_loss: 8.4705 - val_mae: 2.2373\n",
      "Epoch 44/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8310 - mae: 2.1853 - val_loss: 8.2738 - val_mae: 2.2075\n",
      "Epoch 45/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5692 - mae: 2.2848 - val_loss: 8.2246 - val_mae: 2.2074\n",
      "Epoch 46/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5994 - mae: 2.2840 - val_loss: 8.3158 - val_mae: 2.2235\n",
      "Epoch 47/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3599 - mae: 2.2490 - val_loss: 8.2339 - val_mae: 2.2080\n",
      "Epoch 48/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.2507 - mae: 2.2384 - val_loss: 8.3507 - val_mae: 2.2207\n",
      "Epoch 49/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.1598 - mae: 2.2158 - val_loss: 8.2349 - val_mae: 2.2015\n",
      "Epoch 50/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2984 - mae: 2.2605 - val_loss: 8.5251 - val_mae: 2.2338\n",
      "Epoch 51/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5995 - mae: 2.2866 - val_loss: 8.2796 - val_mae: 2.2132\n",
      "Epoch 52/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5240 - mae: 2.3000 - val_loss: 8.1901 - val_mae: 2.1972\n",
      "Epoch 53/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3885 - mae: 2.2617 - val_loss: 8.4520 - val_mae: 2.2479\n",
      "Epoch 54/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.5635 - mae: 2.2967 - val_loss: 8.2091 - val_mae: 2.1971\n",
      "Epoch 55/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.8975 - mae: 2.3285 - val_loss: 8.0766 - val_mae: 2.1885\n",
      "Epoch 56/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3580 - mae: 2.2670 - val_loss: 8.3958 - val_mae: 2.2266\n",
      "Epoch 57/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3202 - mae: 2.2390 - val_loss: 8.2202 - val_mae: 2.2064\n",
      "Epoch 58/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6540 - mae: 2.2948 - val_loss: 8.1027 - val_mae: 2.1918\n",
      "Epoch 59/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.3341 - mae: 2.2411 - val_loss: 8.3667 - val_mae: 2.2144\n",
      "Epoch 60/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.6512 - mae: 2.3198 - val_loss: 8.1405 - val_mae: 2.1942\n",
      "Epoch 61/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.9993 - mae: 2.2205 - val_loss: 8.2713 - val_mae: 2.2069\n",
      "Epoch 62/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2558 - mae: 2.2809 - val_loss: 8.0673 - val_mae: 2.1798\n",
      "Epoch 63/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2347 - mae: 2.2406 - val_loss: 8.0862 - val_mae: 2.1987\n",
      "Epoch 64/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 8.0815 - mae: 2.2333 - val_loss: 8.2457 - val_mae: 2.2014\n",
      "Epoch 65/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0509 - mae: 2.2408 - val_loss: 8.0595 - val_mae: 2.1833\n",
      "Epoch 66/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.4585 - mae: 2.2585 - val_loss: 8.1597 - val_mae: 2.2032\n",
      "Epoch 67/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.4838 - mae: 2.2786 - val_loss: 8.2178 - val_mae: 2.2000\n",
      "Epoch 68/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.5003 - mae: 2.2828 - val_loss: 8.6177 - val_mae: 2.2626\n",
      "Epoch 69/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.9868 - mae: 2.2078 - val_loss: 8.0961 - val_mae: 2.1831\n",
      "Epoch 70/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.1448 - mae: 2.2535 - val_loss: 8.2388 - val_mae: 2.2077\n",
      "Epoch 71/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3671 - mae: 2.2647 - val_loss: 8.3023 - val_mae: 2.2054\n",
      "Epoch 72/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.0044 - mae: 2.1805 - val_loss: 8.2261 - val_mae: 2.2027\n",
      "Epoch 73/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.1762 - mae: 2.2217 - val_loss: 7.9977 - val_mae: 2.1718\n",
      "Epoch 74/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9835 - mae: 2.2000 - val_loss: 8.1670 - val_mae: 2.1924\n",
      "Epoch 75/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8787 - mae: 2.2009 - val_loss: 8.2884 - val_mae: 2.2222\n",
      "Epoch 76/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.2062 - mae: 2.2413 - val_loss: 7.9054 - val_mae: 2.1695\n",
      "Epoch 77/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.9007 - mae: 2.2089 - val_loss: 7.9248 - val_mae: 2.1718\n",
      "Epoch 78/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 7.8208 - mae: 2.2055 - val_loss: 9.2709 - val_mae: 2.3717\n",
      "Epoch 79/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.1816 - mae: 2.2419 - val_loss: 7.9022 - val_mae: 2.1755\n",
      "Epoch 80/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8270 - mae: 2.1699 - val_loss: 8.0575 - val_mae: 2.1808\n",
      "Epoch 81/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.8092 - mae: 2.2117 - val_loss: 7.8135 - val_mae: 2.1733\n",
      "Epoch 82/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.8681 - mae: 2.1890 - val_loss: 7.9950 - val_mae: 2.1809\n",
      "Epoch 83/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.5937 - mae: 2.1713 - val_loss: 7.6913 - val_mae: 2.1454\n",
      "Epoch 84/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.5014 - mae: 2.1832 - val_loss: 8.2299 - val_mae: 2.2084\n",
      "Epoch 85/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.4579 - mae: 2.1502 - val_loss: 7.6984 - val_mae: 2.1609\n",
      "Epoch 86/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.8444 - mae: 2.2025 - val_loss: 7.5653 - val_mae: 2.1334\n",
      "Epoch 87/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7131 - mae: 2.1949 - val_loss: 7.7106 - val_mae: 2.1479\n",
      "Epoch 88/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 7.3867 - mae: 2.1482 - val_loss: 7.5436 - val_mae: 2.1211\n",
      "Epoch 89/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1106 - mae: 2.2494 - val_loss: 8.1817 - val_mae: 2.1897\n",
      "Epoch 90/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.3946 - mae: 2.1412 - val_loss: 8.0283 - val_mae: 2.1725\n",
      "Epoch 91/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.2063 - mae: 2.1042 - val_loss: 7.6949 - val_mae: 2.1407\n",
      "Epoch 92/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.1141 - mae: 2.1077 - val_loss: 7.6796 - val_mae: 2.1620\n",
      "Epoch 93/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0280 - mae: 2.0994 - val_loss: 7.7342 - val_mae: 2.1489\n",
      "Epoch 94/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.2101 - mae: 2.1057 - val_loss: 7.7978 - val_mae: 2.1614\n",
      "Epoch 95/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.5882 - mae: 2.1872 - val_loss: 7.5599 - val_mae: 2.1230\n",
      "Epoch 96/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.5999 - mae: 2.1714 - val_loss: 7.6783 - val_mae: 2.1463\n",
      "Epoch 97/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2163 - mae: 2.1242 - val_loss: 7.6886 - val_mae: 2.1387\n",
      "Epoch 98/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.0969 - mae: 2.0921 - val_loss: 7.7712 - val_mae: 2.1490\n"
     ]
    }
   ],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=10)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    validation_split=0.2, shuffle=True, batch_size = 64, callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "assigned-afternoon",
   "metadata": {
    "id": "assigned-afternoon"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7767 - mae: 1.8653\n",
      "Test Loss: [6.1524505615234375, 1.9186064004898071]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-tract",
   "metadata": {
    "id": "precise-tract"
   },
   "source": [
    "<a name='3.4'></a>\n",
    "## Cuestión 4: ¿En cuáles de estas aplicaciones se usaría un arquitectura 'many-to-one'?\n",
    "\n",
    "**a)** Clasificación de sentimiento en textos\n",
    "\n",
    "**b)** Verificación de voz para iniciar el ordenador.\n",
    "\n",
    "**c)** Generación de música.\n",
    "\n",
    "**d)** Un clasificador que clasifique piezas de música según su autor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-mayor",
   "metadata": {
    "id": "professional-mayor"
   },
   "source": [
    "Opciones a, b y d. En todas ellas se emplean varios inputs (palabras, características de la voz y características de las piezas musicales respectivamente) para dar lugar a una única salida (una clasificación de 'sentimiento', una decisión binaria y una clasificación por autor respectivamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-error",
   "metadata": {
    "id": "fallen-error"
   },
   "source": [
    "<a name='3.5'></a>\n",
    "## Cuestión 5: ¿Qué ventajas aporta el uso de word embeddings?\n",
    "\n",
    "**a)** Permiten reducir la dimensión de entrada respecto al one-hot encoding.\n",
    "\n",
    "**b)** Permiten descubrir la similaridad entre palabras de manera más intuitiva que con one-hot encoding.\n",
    "\n",
    "**c)** Son una manera de realizar transfer learning en nlp.\n",
    "\n",
    "**d)** Permiten visualizar las relaciones entre palabras con métodos de reducción de dimensioones como el PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-polish",
   "metadata": {
    "id": "stylish-polish"
   },
   "source": [
    "Todas las respuestas son correctas:\n",
    "\n",
    "a) Los word embaddings representan las palabras en muchas menos dimensiones que el one-hot encoding.\n",
    "\n",
    "b) Las palabras similares se representan con vectores similares. En cambio con one-hot encoding se construyen vectores ortogonales.\n",
    "\n",
    "c) Los embeddings ya entrenados pueden ser usados en otros campos.\n",
    "\n",
    "d) Verdadero."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
